import pandas as pd
import numpy as np
import shap
import inspect
import os
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestRegressor


# ================================
# è®ºæ–‡çº§ç»˜å›¾å‚æ•°ï¼ˆå…¨å±€ï¼‰
# ================================
plt.rcParams["font.family"] = "Arial"
plt.rcParams["figure.dpi"] = 300
plt.rcParams["axes.titlesize"] = 14
plt.rcParams["axes.labelsize"] = 12
plt.rcParams["xtick.labelsize"] = 11
plt.rcParams["ytick.labelsize"] = 11
plt.rcParams["legend.fontsize"] = 11


# =========================================================
# â˜…â˜… OneHotEncoder é€‚é…æ–°æ—§ç‰ˆæœ¬ sklearn â˜…â˜…
# =========================================================
def make_ohe():
    params = inspect.signature(OneHotEncoder).parameters
    if "sparse_output" in params:
        return OneHotEncoder(handle_unknown="ignore", sparse_output=False)
    else:
        return OneHotEncoder(handle_unknown="ignore", sparse=False)


# =========================================================
# â˜…â˜… æ ¹æ® Alloy æå– 1â€“7 ç³» â˜…â˜…
# =========================================================
def add_series_column(df):
    df = df.copy()
    df["Series"] = df["Alloy"].astype(str).str.extract(r"^(\d)").astype(float)
    df = df[df["Series"].isin([1, 2, 3, 4, 5, 6, 7])]
    df["Series"] = df["Series"].astype(int)
    return df


# =========================================================
# â˜…â˜… è®­ç»ƒæŸ 1 ä¸ª target çš„ 7 ä¸ªç³»åˆ—ä¸“å®¶ + è¿”å› SHAP â˜…â˜…
#      ä½¿ç”¨â€œå…¨å±€ç»Ÿä¸€çš„ OHE + StandardScalerâ€
# =========================================================
def train_experts_and_get_shap(excel_file, sheet_name, target_col):

    df = pd.read_excel(excel_file, sheet_name=sheet_name)
    df = add_series_column(df)

    y_all = df[target_col]
    feature_cols = [c for c in df.columns if c not in ["Series", "Alloy", target_col]]
    X_all = df[feature_cols]
    series_all = df["Series"]

    # æ•°å€¼ / ç±»åˆ«ç‰¹å¾
    num_cols = X_all.select_dtypes(include=[np.number]).columns.tolist()
    cat_cols = [c for c in X_all.columns if c not in num_cols]

    # ---------- â˜… å…¨å±€é¢„å¤„ç†å™¨ï¼šåœ¨æ•´å¼ è¡¨ä¸Š fit â˜… ----------
    ohe = make_ohe()
    preprocess = ColumnTransformer(
        transformers=[
            ("num", StandardScaler(), num_cols),
            ("cat", ohe, cat_cols),
        ]
    )
    preprocess.fit(X_all)   # â˜…â˜… åœ¨å…¨ä½“æ•°æ®ä¸Šç»Ÿä¸€ fit OHEï¼Œä¿è¯ç»´åº¦ä¸€è‡´

    # -------------------------------
    # åˆ†å±‚æŠ½æ ·
    # -------------------------------
    (
        X_train, X_test,
        y_train, y_test,
        series_train, series_test
    ) = train_test_split(
        X_all, y_all, series_all,
        test_size=0.2,
        random_state=42,
        stratify=series_all
    )

    # å…¨å±€ transform
    X_train_pre = preprocess.transform(X_train)

    # ç‰¹å¾å
    num_f = list(num_cols)

    if len(cat_cols) > 0:
        try:
            ohe_f = list(ohe.get_feature_names_out(cat_cols))
        except:
            ohe_f = []
    else:
        ohe_f = []

    feature_names = num_f + ohe_f

    # -------------------------
    # SHAP æ±‡æ€»å®¹å™¨
    # -------------------------
    shap_dict = {}
    series_train_arr = series_train.values

    # -------------------------
    # æ¯ä¸ª Series è®­ç»ƒä¸“å®¶æ¨¡å‹ + SHAP
    # -------------------------
    for s in range(1, 8):

        idx_s = np.where(series_train_arr == s)[0]
        if len(idx_s) < 2:
            print(f"Series {s}: æ ·æœ¬ä¸è¶³ï¼ˆ{len(idx_s)}ï¼‰ï¼Œè·³è¿‡ã€‚")
            continue

        X_s = X_train_pre[idx_s]
        y_s = y_train.iloc[idx_s]

        rf = RandomForestRegressor(
            n_estimators=300,
            random_state=42
        )
        rf.fit(X_s, y_s)

        explainer = shap.TreeExplainer(rf)
        shap_vals = explainer.shap_values(X_s)

        shap_dict[s] = np.abs(shap_vals).mean(axis=0)

        print(f"Series {s}: SHAP å·²è®¡ç®—å®Œæˆï¼Œæ ·æœ¬æ•° = {len(idx_s)}")

    return shap_dict, feature_names


# =========================================================
# â˜…â˜… è®ºæ–‡çº§åˆ« SHAP åˆ†ç»„å›¾ï¼ˆSeries1â€“3 vs Series4â€“7ï¼‰â˜…â˜…
# =========================================================
def plot_grouped_shap(shap_dict, feature_names, title, save_prefix):

    os.makedirs("shap_figures", exist_ok=True)

    if not shap_dict:
        print(f"{title}: æ— å¯ç”¨ SHAP æ•°æ®ï¼Œè·³è¿‡ã€‚")
        return

    series_ids = sorted(shap_dict.keys())

    # åˆå¹¶ SHAP çŸ©é˜µ
    shap_matrix = np.vstack([shap_dict[s] for s in series_ids])
    n_shap_features = shap_matrix.shape[1]
    n_name_features = len(feature_names)

    # ç‰¹å¾åè¡¥é½ï¼ˆå¤„ç† OHE transform æ—¶å‡ºç°çš„ unseen categoriesï¼‰
    if n_name_features < n_shap_features:
        extra_names = [f"Feature_{i}" for i in range(n_name_features, n_shap_features)]
        feature_names_extended = feature_names + extra_names
    else:
        feature_names_extended = feature_names

    # Top K ç‰¹å¾
    top_k = min(12, n_shap_features)
    mean_importance = shap_matrix.mean(axis=0)
    top_idx = np.argsort(mean_importance)[-top_k:][::-1]
    top_features = [feature_names_extended[i] for i in top_idx]

    # Series 1â€“3 vs 4â€“7
    groupA_rows = [shap_dict[s] for s in series_ids if s in (1, 2, 3)]
    groupB_rows = [shap_dict[s] for s in series_ids if s in (4, 5, 6, 7)]

    group_A = np.vstack(groupA_rows).mean(axis=0)[top_idx] if groupA_rows else np.zeros(top_k)
    group_B = np.vstack(groupB_rows).mean(axis=0)[top_idx] if groupB_rows else np.zeros(top_k)

    # ç»˜å›¾
    y = np.arange(len(top_features))
    fig, ax = plt.subplots(figsize=(8, 6))

    ax.barh(y - 0.18, group_A, height=0.35,
            color="#D62728", label="Series 1â€“3", alpha=0.9)
    ax.barh(y + 0.18, group_B, height=0.35,
            color="#FF9896", label="Series 4â€“7", alpha=0.9)

    ax.set_yticks(y)
    ax.set_yticklabels(top_features)
    ax.set_xlabel("mean(|SHAP value|)")
    ax.set_title(title)
    ax.legend(frameon=False)
    ax.invert_yaxis()

    plt.tight_layout()
    plt.savefig(f"shap_figures/{save_prefix}.png", dpi=300)
    plt.savefig(f"shap_figures/{save_prefix}.pdf")
    plt.close()

    print(f"âœ” å·²ç”Ÿæˆè®ºæ–‡çº§å›¾åƒï¼šshap_figures/{save_prefix}.png")


# =========================================================
# â˜…â˜… ä¸»æµç¨‹ â€” è®­ç»ƒä¸‰å¤§ç±»å¹¶ç»˜å›¾ â˜…â˜…
# =========================================================
if __name__ == "__main__":
    excel_file = "YTS UTS EL sheet.xlsx"

    targets = [
        ("UTS", "UTS"),
        ("YTS", "YTS"),
        ("EL",  "EL")
    ]

    for sheet_name, target in targets:
        print(f"\n===== å¼€å§‹å¤„ç† {target} =====")

        shap_dict, feature_names = train_experts_and_get_shap(
            excel_file, sheet_name, target
        )

        plot_grouped_shap(
            shap_dict,
            feature_names,
            f"{target} â€” SHAP Grouped Importance",
            f"{target}_SHAP_grouped"
        )

    print("\nğŸ‰ æ‰€æœ‰ SHAP å›¾å·²ç»ç”Ÿæˆå®Œæ¯•ï¼ˆ3 å¼  PNG + 3 å¼  PDFï¼‰")
